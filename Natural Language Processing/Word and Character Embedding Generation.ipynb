{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/vvsaripalli/SECReports/Understanding-Financial-Reports-using-Natural-Language-Processing/Natural Language Processing/wordembeddings/lib/python3.5/site-packages\n",
      "gensim installed\n"
     ]
    }
   ],
   "source": [
    "import pandas \n",
    "import numpy\n",
    "import glob\n",
    "import gensim\n",
    "import logging\n",
    "import sys, os\n",
    "import nltk\n",
    "import inflect\n",
    "import itertools\n",
    "import re, string, unicodedata\n",
    "import multiprocessing \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "from time import time  # To time our operations\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "\n",
    "# */site-packages is where your current session is running its python out of\n",
    "site_path = ''\n",
    "for path in sys.path:\n",
    "    if 'site-packages' in path.split('/')[-1]:\n",
    "        print(path)\n",
    "        site_path = path\n",
    "# search to see if gensim in installed packages\n",
    "if len(site_path) > 0:\n",
    "    if not 'gensim' in os.listdir(site_path):\n",
    "        print('package not found')\n",
    "    else:\n",
    "        print('gensim installed')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all the text files in the corpus and tokeniztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCorpus(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        line = f.read().decode('utf-8')\n",
    "    return line\n",
    "\n",
    "\n",
    "def normalize():\n",
    "    final_words = []\n",
    "    for filename in glob.glob('/home2/vvsaripalli/Final_Corpus_N-CSR/*.txt'):\n",
    "        words = readCorpus(filename)\n",
    "        final_words = \" \".join(words)\n",
    "    tokenizer_words = TweetTokenizer()\n",
    "    tokens_sentences = [tokenizer_words.tokenize(t) for t in \n",
    "    nltk.sent_tokenize(final)]\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training our Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the necessary hyperparameteres to tunr our word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 200 # dimensions of each word embedding\n",
    "min_word_count = 1 # this is not advisable but since we need to extract\n",
    "# feature vector for each word we need to do this\n",
    "num_workers = multiprocessing.cpu_count() # number of threads running in parallel\n",
    "context_size = 7 # context window length\n",
    "downsampling = 1e-3 # downsampling for very frequent words\n",
    "seed = 1 # seed for random number generator to make results reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defining our Word2Vec model with the above declared hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_ = Word2Vec(\n",
    "    sg = 1, seed = seed,\n",
    "    workers = num_workers,\n",
    "    size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = context_size,\n",
    "    sample = downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that we train our vocabulary first before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_.build_vocab(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now training the Word2Vec model with the vocabulary generated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_.train(data, total_examples = word2vec_.corpus_count, epochs = word2vec_.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2vec_.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_.most_similar('credit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the entire vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(word2vec_.wv.vocab.keys())\n",
    "vocab[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
