{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/vvsaripalli/SECReports/Understanding-Financial-Reports-using-Natural-Language-Processing/Natural Language Processing/wordembeddings/lib/python3.5/site-packages\n",
      "gensim installed\n",
      "TensorFlow version: \t1.12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas \n",
    "import numpy\n",
    "import gensim\n",
    "import logging\n",
    "import sys, os\n",
    "import nltk\n",
    "import re, string, unicodedata\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time  # To time our operations\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# */site-packages is where your current session is running its python out of\n",
    "site_path = ''\n",
    "for path in sys.path:\n",
    "    if 'site-packages' in path.split('/')[-1]:\n",
    "        print(path)\n",
    "        site_path = path\n",
    "# search to see if gensim in installed packages\n",
    "if len(site_path) > 0:\n",
    "    if not 'gensim' in os.listdir(site_path):\n",
    "        print('package not found')\n",
    "    else:\n",
    "        print('gensim installed')    \n",
    "        \n",
    "\n",
    "# Checking tensorflow installation\n",
    "print('TensorFlow version: \\t%s' % tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining directories for reading text files and saving checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For displaying gensim logs\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Directory with raw txt-files\n",
    "TEXT_DIR  = '/home2/vvsaripalli/WordEmbeddingCorpus/'\n",
    "\n",
    "# Directory for saving checkpoint and metadata\n",
    "MODEL_DIR = 'Checkpoints/'\n",
    "\n",
    "# Word2vec\n",
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all the text files in the corpus and tokeniztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    \"\"\"\n",
    "    Read in text files\n",
    "    \"\"\"\n",
    "    documents = list()\n",
    "    tokenize  = lambda x: simple_preprocess(x)\n",
    "    \n",
    "    # Read in all files in directory\n",
    "    if os.path.isdir(path):\n",
    "        for filename in os.listdir(path):\n",
    "            with open('%s/%s' % (path, filename), encoding='utf-8') as f:\n",
    "                doc = f.read()\n",
    "                doc = clean_doc(doc)\n",
    "                documents.append(tokenize(doc))\n",
    "    return documents\n",
    "\n",
    "def clean_doc(doc):\n",
    "    \"\"\"\n",
    "    Cleaning a document by several methods\n",
    "    \"\"\"\n",
    "    #doing basic cleaning\n",
    "    doc = re.sub(r'\\-+', '.', doc)\n",
    "    doc = re.sub(r'\\=+', '', doc)\n",
    "    doc = re.sub(r'\\(+', '', doc)\n",
    "    doc = re.sub(r'\\)+', '', doc)\n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    #Replace fullstop with token to train on\n",
    "    doc = doc.replace('.', ' __PERIOD__ ')\n",
    "    # eplace numbers with token to train on\n",
    "    doc = re.sub(r\"[0-9]+\", \"__NUMBER__\", doc)\n",
    "    # Remove ' and full stops and brackets\n",
    "    doc = re.sub(r'[{}()\\']', '', doc)\n",
    "    #Remove other special characters\n",
    "    doc = re.sub(r'[;,:-@#]', '', doc)\n",
    "    # Split in tokens\n",
    "    tokens = doc.split()\n",
    "    # Tokens with less then two characters will be ignored\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4361\n"
     ]
    }
   ],
   "source": [
    "docs = read_files(TEXT_DIR)\n",
    "print('Number of documents: %i' % len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training our Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the necessary hyperparameteres to tunr our word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defining and training our Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : collected 36515 word types from a corpus of 20113566 raw words and 4361 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : effective_min_count=5 retains 20985 unique words (57% of original 36515, drops 15530)\n",
      "INFO : effective_min_count=5 leaves 20081026 word corpus (99% of original 20113566, drops 32540)\n",
      "INFO : deleting the raw counts dictionary of 36515 items\n",
      "INFO : sample=0.001 downsamples 71 most-common words\n",
      "INFO : downsampling leaves estimated 14251841 word corpus (71.0% of prior 20081026)\n",
      "INFO : estimated required memory for 20985 words and 300 dimensions: 60856500 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 3 workers on 20985 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : EPOCH 1 - PROGRESS: at 10.75% examples, 1058743 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 22.11% examples, 1077651 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 35.20% examples, 1085447 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 45.61% examples, 1087948 words/s, in_qsize 6, out_qsize 1\n",
      "INFO : EPOCH 1 - PROGRESS: at 57.62% examples, 1089718 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 69.82% examples, 1092053 words/s, in_qsize 6, out_qsize 1\n",
      "INFO : EPOCH 1 - PROGRESS: at 82.55% examples, 1094058 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 1 - PROGRESS: at 94.36% examples, 1094155 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 20113566 raw words (9301297 effective words) took 8.5s, 1097160 effective words/s\n",
      "INFO : EPOCH 2 - PROGRESS: at 11.37% examples, 1122836 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 23.34% examples, 1132543 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 36.41% examples, 1128411 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 48.31% examples, 1132653 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 59.89% examples, 1133018 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 73.08% examples, 1135250 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 86.29% examples, 1135728 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 2 - PROGRESS: at 98.42% examples, 1135173 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 20113566 raw words (9300857 effective words) took 8.2s, 1136350 effective words/s\n",
      "INFO : EPOCH 3 - PROGRESS: at 11.58% examples, 1122632 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 23.53% examples, 1133456 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 36.41% examples, 1126410 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 48.31% examples, 1132650 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 59.89% examples, 1133344 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 71.86% examples, 1123389 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 85.07% examples, 1125120 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 3 - PROGRESS: at 97.64% examples, 1127138 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 20113566 raw words (9301233 effective words) took 8.2s, 1128661 effective words/s\n",
      "INFO : EPOCH 4 - PROGRESS: at 11.37% examples, 1125315 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 23.32% examples, 1128230 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 36.16% examples, 1124294 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 47.99% examples, 1127655 words/s, in_qsize 6, out_qsize 2\n",
      "INFO : EPOCH 4 - PROGRESS: at 59.73% examples, 1130043 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 72.74% examples, 1132822 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 85.65% examples, 1131041 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 4 - PROGRESS: at 97.91% examples, 1130357 words/s, in_qsize 4, out_qsize 1\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 20113566 raw words (9301136 effective words) took 8.2s, 1131152 effective words/s\n",
      "INFO : EPOCH 5 - PROGRESS: at 11.30% examples, 1101791 words/s, in_qsize 4, out_qsize 1\n",
      "INFO : EPOCH 5 - PROGRESS: at 22.93% examples, 1116192 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 35.98% examples, 1116360 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 46.96% examples, 1117580 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 58.56% examples, 1114410 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 71.11% examples, 1113509 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 84.06% examples, 1114062 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : EPOCH 5 - PROGRESS: at 95.96% examples, 1113004 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 20113566 raw words (9300661 effective words) took 8.4s, 1113800 effective words/s\n",
      "INFO : training on a 100567830 raw words (46505184 effective words) took 41.5s, 1120029 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(docs, size=EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our trained model as a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving Word2Vec object under Checkpoints/word2vec, separately None\n",
      "INFO : not storing attribute vectors_norm\n",
      "INFO : not storing attribute cum_table\n",
      "INFO : saved Checkpoints/word2vec\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "model.save(os.path.join(MODEL_DIR,'word2vec'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating metadata and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights: (20985, 300)\n",
      "Vocabulary size: 20985\n",
      "Embedding size: 300\n"
     ]
    }
   ],
   "source": [
    "weights     = model.wv.vectors\n",
    "index_words = model.wv.index2word\n",
    "\n",
    "vocab_size    = weights.shape[0]\n",
    "embedding_dim = weights.shape[1]\n",
    "\n",
    "print('Shape of weights:', weights.shape)\n",
    "print('Vocabulary size: %i' % vocab_size)\n",
    "print('Embedding size: %i'  % embedding_dim)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR,'metadata.tsv'), 'w') as f:\n",
    "    f.writelines(\"\\n\".join(index_words))\n",
    "\n",
    "# Required if you re-run without restarting the kernel\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "\n",
    "embedding_init = W.assign(embedding_placeholder)\n",
    "writer = tf.summary.FileWriter(MODEL_DIR, graph=tf.get_default_graph())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = W.name\n",
    "embedding.metadata_path = './metadata.tsv'\n",
    "projector.visualize_embeddings(writer, config)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: weights})\n",
    "    save_path = saver.save(sess, os.path.join(MODEL_DIR, \"model.cpkt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('receive', 0.6883692741394043),\n",
       " ('rate', 0.3697010278701782),\n",
       " ('floating', 0.36679479479789734),\n",
       " ('usd', 0.35608696937561035),\n",
       " ('pays', 0.34642866253852844),\n",
       " ('receiving', 0.34415680170059204),\n",
       " ('krw', 0.33488595485687256),\n",
       " ('paying', 0.3230683505535126),\n",
       " ('sep', 0.32099348306655884),\n",
       " ('pribor', 0.30752915143966675)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['pay'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
